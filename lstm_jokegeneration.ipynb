{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this notebook i play with a sample i found before.\n",
    "#this pytorch sample creates text from logits a random   probable item at next iteration \n",
    "#choice is not totally random nor totally persistent\n",
    "#result of torch.nn.functional.softmax is given to choice as parameter so it chooses a semi random item\n",
    "#method return logits without applying any transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What did the bartender say to the jumper cable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't you hate jokes about German sausage? The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Two artists had an art contest... It ended in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Why did the chicken cross the playground? To g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What gun do you use to hunt a moose? A moosecut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>If life gives you melons, you might have dysle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Broken pencils... ...are pointless.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>What did one snowman say to the other snowman?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>How many hipsters does it take to change a lig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Where do sick boats go? The dock!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  What did the bartender say to the jumper cable...\n",
       "1   2  Don't you hate jokes about German sausage? The...\n",
       "2   3  Two artists had an art contest... It ended in ...\n",
       "3   4  Why did the chicken cross the playground? To g...\n",
       "4   5   What gun do you use to hunt a moose? A moosecut!\n",
       "5   6  If life gives you melons, you might have dysle...\n",
       "6   7                Broken pencils... ...are pointless.\n",
       "7   8  What did one snowman say to the other snowman?...\n",
       "8   9  How many hipsters does it take to change a lig...\n",
       "9  10                  Where do sick boats go? The dock!"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('D:/data7/reddit-cleanjokes.csv')\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What did the bartender say to the jumper cable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Don't you hate jokes about German sausage? The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Two artists had an art contest... It ended in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Why did the chicken cross the playground? To g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What gun do you use to hunt a moose? A moosecut!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>If life gives you melons, you might have dysle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Broken pencils... ...are pointless.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>What did one snowman say to the other snowman?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>How many hipsters does it take to change a lig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Where do sick boats go? The dock!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Joke\n",
       "0   1  What did the bartender say to the jumper cable...\n",
       "1   2  Don't you hate jokes about German sausage? The...\n",
       "2   3  Two artists had an art contest... It ended in ...\n",
       "3   4  Why did the chicken cross the playground? To g...\n",
       "4   5   What gun do you use to hunt a moose? A moosecut!\n",
       "5   6  If life gives you melons, you might have dysle...\n",
       "6   7                Broken pencils... ...are pointless.\n",
       "7   8  What did one snowman say to the other snowman?...\n",
       "8   9  How many hipsters does it take to change a lig...\n",
       "9  10                  Where do sick boats go? The dock!"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Joke'].str.cat(sep=' ')\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StepLogger():\n",
    "    def __init__(self,capacity):\n",
    "        self.tensor_datas = {}        \n",
    "        self.capacity = capacity\n",
    "        self.added_labels = []\n",
    "        \n",
    "    \n",
    "    def add_info(self,tensor_data,tensor_label):\n",
    "        if tensor_label not in self.added_labels:\n",
    "            self.added_labels.append( tensor_label )\n",
    "        \n",
    "        if tensor_label in self.tensor_datas.keys():\n",
    "            current_arr = self.tensor_datas.get(tensor_label)\n",
    "            if len(current_arr) < self.capacity:\n",
    "                current_arr = self.tensor_datas.get(tensor_label, [])\n",
    "                current_arr.append(tensor_data)\n",
    "        else:\n",
    "            self.tensor_datas[tensor_label] = [tensor_data]\n",
    "    \n",
    "    def get_default_summary(self,show_data=False,summary_count=1):\n",
    "        self.get_summary(self.added_labels,show_data,summary_count)\n",
    "        \n",
    "    def get_summary(self,labels,show_data=False,summary_count=1):\n",
    "        print(\"summary_count\",summary_count,\"   self.capacity \",self.capacity)\n",
    "        count = 0\n",
    "        for i in range(summary_count):\n",
    "            print(i,\" ------------------------------------------------\")\n",
    "            for l in labels:\n",
    "                label_data = self.tensor_datas.get(l)[i]\n",
    "                print(l)\n",
    "                if torch.is_tensor(label_data):\n",
    "                    print( list(label_data.size() ) )\n",
    "                if not show_data and not torch.is_tensor(label_data):\n",
    "                    print(label_data)\n",
    "                if show_data:    \n",
    "                    print(label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(Model, self).__init__()\n",
    "        self.lstm_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.lstm_size,\n",
    "            hidden_size=self.lstm_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.lstm_size, n_vocab)\n",
    "\n",
    "    def forward(self, x, prev_state):\n",
    "        step_logger.add_info(x,\"forward x\")\n",
    "        \n",
    "        embed = self.embedding(x)\n",
    "        step_logger.add_info(embed,\"forward embed\")\n",
    "        \n",
    "        output, state = self.lstm(embed, prev_state)\n",
    "        step_logger.add_info(output,\"forward output\")\n",
    "        step_logger.add_info(state,\"forward state\")\n",
    "        \n",
    "        logits = self.fc(output)\n",
    "        step_logger.add_info(logits,\"forward logits\")\n",
    "\n",
    "        return logits, state\n",
    "\n",
    "    def init_state(self, sequence_length):\n",
    "        return (torch.zeros(self.num_layers, sequence_length, self.lstm_size),\n",
    "                torch.zeros(self.num_layers, sequence_length, self.lstm_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,sequence_length,):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self):\n",
    "        train_df = pd.read_csv('D:/data7/reddit-cleanjokes.csv')\n",
    "        text = train_df['Joke'].str.cat(sep=' ')\n",
    "        return text.split(' ')\n",
    "\n",
    "    def get_uniq_words(self):\n",
    "        word_counts = Counter(self.words)\n",
    "        return sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            torch.tensor(self.words_indexes[index:index+sequence_length]),\n",
    "            torch.tensor(self.words_indexes[index+1:index+sequence_length+1]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, model, batch_size,sequence_length,max_epochs):\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset,batch_size=batch_size,)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        state_h, state_c = model.init_state(sequence_length)\n",
    "\n",
    "        for batch, (x, y) in enumerate(dataloader):\n",
    "\n",
    "            step_logger.add_info(x,\"epoch x\")\n",
    "            step_logger.add_info(y,\"epoch y\")\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "            step_logger.add_info(y_pred,\"model y_pred\")\n",
    "            step_logger.add_info(state_h,\"model state_h\")\n",
    "            step_logger.add_info(state_c,\"model state_c\")\n",
    "            \n",
    "            loss = criterion(y_pred.transpose(1, 2), y)\n",
    "            step_logger.add_info(loss,\"model loss\")\n",
    "\n",
    "            state_h = state_h.detach()\n",
    "            state_c = state_c.detach()\n",
    "            \n",
    "            step_logger.add_info(state_h,\"model state_h\")\n",
    "            step_logger.add_info(state_c,\"model state_c\")\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print({ 'epoch': epoch, 'batch': batch, 'loss': loss.item() })\n",
    "\n",
    "def predict(dataset, model, text, next_words=100,use_max_possible=False):\n",
    "    words = text.split(' ')\n",
    "    model.eval()\n",
    "\n",
    "    state_h, state_c = model.init_state(len(words))\n",
    "\n",
    "    for i in range(0, next_words):\n",
    "        x = torch.tensor([[dataset.word_to_index[w] for w in words[i:]]])\n",
    "        step_logger.add_info(x,\"predict x\")\n",
    "        \n",
    "        y_pred, (state_h, state_c) = model(x, (state_h, state_c))\n",
    "        step_logger.add_info(y_pred,\"predict y_pred\")\n",
    "        step_logger.add_info(state_h,\"predict state_h\")\n",
    "        step_logger.add_info(state_c,\"predict state_c\")\n",
    "\n",
    "        last_word_logits = y_pred[0][-1]\n",
    "        step_logger.add_info(last_word_logits,\"predict last_word_logits\")\n",
    "        \n",
    "        p = torch.nn.functional.softmax(last_word_logits, dim=0).detach().numpy()\n",
    "        step_logger.add_info(p,\"predict p\")\n",
    "\n",
    "        \n",
    "        word_index = np.random.choice(len(last_word_logits), p=p)\n",
    "        step_logger.add_info(word_index,\"predict word_index\")\n",
    "        if use_max_possible :\n",
    "            word_index = np.argmax(p)\n",
    "            \n",
    "        \n",
    "        words.append(dataset.index_to_word[word_index])\n",
    "        step_logger.add_info(dataset.index_to_word[word_index],\"predict dataset.index_to_word[word_index]\")\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_logger =  StepLogger(2)\n",
    "\n",
    "sequence_length = 4\n",
    "dataset = Dataset(sequence_length)\n",
    "model = Model(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(6925, 128)\n",
      "  (lstm): LSTM(128, 128, num_layers=3, dropout=0.2)\n",
      "  (fc): Linear(in_features=128, out_features=6925, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x tensor([[  2,   8,   0, 248],\n",
      "        [  8,   0, 248,  20],\n",
      "        [  0, 248,  20,   4],\n",
      "        [248,  20,   4,   0]])\n",
      "y tensor([[   8,    0,  248,   20],\n",
      "        [   0,  248,   20,    4],\n",
      "        [ 248,   20,    4,    0],\n",
      "        [  20,    4,    0, 1905]])\n",
      "\n",
      "x tensor([[  20,    4,    0, 1905],\n",
      "        [   4,    0, 1905, 1906],\n",
      "        [   0, 1905, 1906,   64],\n",
      "        [1905, 1906,   64,  534]])\n",
      "y tensor([[   4,    0, 1905, 1906],\n",
      "        [   0, 1905, 1906,   64],\n",
      "        [1905, 1906,   64,  534],\n",
      "        [1906,   64,  534,   73]])\n",
      "\n",
      "x tensor([[1906,   64,  534,   73],\n",
      "        [  64,  534,   73,  535],\n",
      "        [ 534,   73,  535,    4],\n",
      "        [  73,  535,    4, 1907]])\n",
      "y tensor([[  64,  534,   73,  535],\n",
      "        [ 534,   73,  535,    4],\n",
      "        [  73,  535,    4, 1907],\n",
      "        [ 535,    4, 1907, 1908]])\n",
      "\n",
      "x tensor([[ 535,    4, 1907, 1908],\n",
      "        [   4, 1907, 1908,  225],\n",
      "        [1907, 1908,  225,    3],\n",
      "        [1908,  225,    3,  226]])\n",
      "y tensor([[   4, 1907, 1908,  225],\n",
      "        [1907, 1908,  225,    3],\n",
      "        [1908,  225,    3,  226],\n",
      "        [ 225,    3,  226,  227]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset,batch_size=4)\n",
    "for batch, (x, y) in enumerate(dataloader):\n",
    "    if batch < 4:\n",
    "        print(\"x\", x)\n",
    "        print(\"y\", y)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'batch': 0, 'loss': 8.852972030639648}\n",
      "{'epoch': 0, 'batch': 1, 'loss': 8.851600646972656}\n",
      "{'epoch': 0, 'batch': 2, 'loss': 8.838411331176758}\n",
      "{'epoch': 0, 'batch': 3, 'loss': 8.834159851074219}\n",
      "{'epoch': 0, 'batch': 4, 'loss': 8.8235445022583}\n",
      "{'epoch': 0, 'batch': 5, 'loss': 8.825240135192871}\n",
      "{'epoch': 0, 'batch': 6, 'loss': 8.818626403808594}\n",
      "{'epoch': 0, 'batch': 7, 'loss': 8.799959182739258}\n",
      "{'epoch': 0, 'batch': 8, 'loss': 8.786689758300781}\n",
      "{'epoch': 0, 'batch': 9, 'loss': 8.756302833557129}\n",
      "{'epoch': 0, 'batch': 10, 'loss': 8.71680736541748}\n",
      "{'epoch': 0, 'batch': 11, 'loss': 8.64470386505127}\n",
      "{'epoch': 0, 'batch': 12, 'loss': 8.556174278259277}\n",
      "{'epoch': 0, 'batch': 13, 'loss': 8.465197563171387}\n",
      "{'epoch': 0, 'batch': 14, 'loss': 8.262255668640137}\n",
      "{'epoch': 0, 'batch': 15, 'loss': 8.12722110748291}\n",
      "{'epoch': 0, 'batch': 16, 'loss': 7.903187274932861}\n",
      "{'epoch': 0, 'batch': 17, 'loss': 7.840312957763672}\n",
      "{'epoch': 0, 'batch': 18, 'loss': 7.749034881591797}\n",
      "{'epoch': 0, 'batch': 19, 'loss': 7.6450347900390625}\n",
      "{'epoch': 0, 'batch': 20, 'loss': 7.405329704284668}\n",
      "{'epoch': 0, 'batch': 21, 'loss': 7.671999454498291}\n",
      "{'epoch': 0, 'batch': 22, 'loss': 7.511297702789307}\n",
      "{'epoch': 0, 'batch': 23, 'loss': 7.6050615310668945}\n",
      "{'epoch': 0, 'batch': 24, 'loss': 7.515978813171387}\n",
      "{'epoch': 0, 'batch': 25, 'loss': 7.338580131530762}\n",
      "{'epoch': 0, 'batch': 26, 'loss': 7.129247188568115}\n",
      "{'epoch': 0, 'batch': 27, 'loss': 7.211191654205322}\n",
      "{'epoch': 0, 'batch': 28, 'loss': 7.736649990081787}\n",
      "{'epoch': 0, 'batch': 29, 'loss': 7.784544944763184}\n",
      "{'epoch': 0, 'batch': 30, 'loss': 7.020479202270508}\n",
      "{'epoch': 0, 'batch': 31, 'loss': 7.0567450523376465}\n",
      "{'epoch': 0, 'batch': 32, 'loss': 7.208972454071045}\n",
      "{'epoch': 0, 'batch': 33, 'loss': 7.431694030761719}\n",
      "{'epoch': 0, 'batch': 34, 'loss': 7.4122490882873535}\n",
      "{'epoch': 0, 'batch': 35, 'loss': 7.74210786819458}\n",
      "{'epoch': 0, 'batch': 36, 'loss': 7.666107654571533}\n",
      "{'epoch': 0, 'batch': 37, 'loss': 7.359927177429199}\n",
      "{'epoch': 0, 'batch': 38, 'loss': 7.662800312042236}\n",
      "{'epoch': 0, 'batch': 39, 'loss': 7.501194953918457}\n",
      "{'epoch': 0, 'batch': 40, 'loss': 7.831523895263672}\n",
      "{'epoch': 0, 'batch': 41, 'loss': 7.440464019775391}\n",
      "{'epoch': 0, 'batch': 42, 'loss': 7.763245105743408}\n",
      "{'epoch': 0, 'batch': 43, 'loss': 7.43780517578125}\n",
      "{'epoch': 0, 'batch': 44, 'loss': 7.345088481903076}\n",
      "{'epoch': 0, 'batch': 45, 'loss': 7.531094074249268}\n",
      "{'epoch': 0, 'batch': 46, 'loss': 7.656784534454346}\n",
      "{'epoch': 0, 'batch': 47, 'loss': 7.964529037475586}\n",
      "{'epoch': 0, 'batch': 48, 'loss': 7.367037773132324}\n",
      "{'epoch': 0, 'batch': 49, 'loss': 7.681538105010986}\n",
      "{'epoch': 0, 'batch': 50, 'loss': 7.907405853271484}\n",
      "{'epoch': 0, 'batch': 51, 'loss': 7.6488037109375}\n",
      "{'epoch': 0, 'batch': 52, 'loss': 7.155616760253906}\n",
      "{'epoch': 0, 'batch': 53, 'loss': 7.459121227264404}\n",
      "{'epoch': 0, 'batch': 54, 'loss': 7.291818618774414}\n",
      "{'epoch': 0, 'batch': 55, 'loss': 7.3746819496154785}\n",
      "{'epoch': 0, 'batch': 56, 'loss': 7.4890456199646}\n",
      "{'epoch': 0, 'batch': 57, 'loss': 7.5013813972473145}\n",
      "{'epoch': 0, 'batch': 58, 'loss': 7.432504653930664}\n",
      "{'epoch': 0, 'batch': 59, 'loss': 7.431922912597656}\n",
      "{'epoch': 0, 'batch': 60, 'loss': 7.328115940093994}\n",
      "{'epoch': 0, 'batch': 61, 'loss': 7.516005516052246}\n",
      "{'epoch': 0, 'batch': 62, 'loss': 7.479032516479492}\n",
      "{'epoch': 0, 'batch': 63, 'loss': 7.393070220947266}\n",
      "{'epoch': 0, 'batch': 64, 'loss': 7.520249843597412}\n",
      "{'epoch': 0, 'batch': 65, 'loss': 7.447807788848877}\n",
      "{'epoch': 0, 'batch': 66, 'loss': 7.394015789031982}\n",
      "{'epoch': 0, 'batch': 67, 'loss': 7.204174995422363}\n",
      "{'epoch': 0, 'batch': 68, 'loss': 7.407392501831055}\n",
      "{'epoch': 0, 'batch': 69, 'loss': 7.110424995422363}\n",
      "{'epoch': 0, 'batch': 70, 'loss': 7.582547187805176}\n",
      "{'epoch': 0, 'batch': 71, 'loss': 7.527458667755127}\n",
      "{'epoch': 0, 'batch': 72, 'loss': 7.40471887588501}\n",
      "{'epoch': 0, 'batch': 73, 'loss': 7.48990535736084}\n",
      "{'epoch': 0, 'batch': 74, 'loss': 7.523906230926514}\n",
      "{'epoch': 0, 'batch': 75, 'loss': 7.655673503875732}\n",
      "{'epoch': 0, 'batch': 76, 'loss': 7.396787643432617}\n",
      "{'epoch': 0, 'batch': 77, 'loss': 7.680214881896973}\n",
      "{'epoch': 0, 'batch': 78, 'loss': 7.807300567626953}\n",
      "{'epoch': 0, 'batch': 79, 'loss': 7.025155067443848}\n",
      "{'epoch': 0, 'batch': 80, 'loss': 7.347019195556641}\n",
      "{'epoch': 0, 'batch': 81, 'loss': 7.5238165855407715}\n",
      "{'epoch': 0, 'batch': 82, 'loss': 7.586792945861816}\n",
      "{'epoch': 0, 'batch': 83, 'loss': 7.6067328453063965}\n",
      "{'epoch': 0, 'batch': 84, 'loss': 7.37197732925415}\n",
      "{'epoch': 0, 'batch': 85, 'loss': 7.609007358551025}\n",
      "{'epoch': 0, 'batch': 86, 'loss': 7.310414791107178}\n",
      "{'epoch': 0, 'batch': 87, 'loss': 7.4301042556762695}\n",
      "{'epoch': 0, 'batch': 88, 'loss': 7.302781581878662}\n",
      "{'epoch': 0, 'batch': 89, 'loss': 7.431869983673096}\n",
      "{'epoch': 0, 'batch': 90, 'loss': 7.778749465942383}\n",
      "{'epoch': 0, 'batch': 91, 'loss': 7.2707624435424805}\n",
      "{'epoch': 0, 'batch': 92, 'loss': 7.518054008483887}\n",
      "{'epoch': 0, 'batch': 93, 'loss': 7.145758628845215}\n",
      "{'epoch': 1, 'batch': 0, 'loss': 7.171810150146484}\n",
      "{'epoch': 1, 'batch': 1, 'loss': 7.166049003601074}\n",
      "{'epoch': 1, 'batch': 2, 'loss': 7.121749401092529}\n",
      "{'epoch': 1, 'batch': 3, 'loss': 7.328856945037842}\n",
      "{'epoch': 1, 'batch': 4, 'loss': 7.297666072845459}\n",
      "{'epoch': 1, 'batch': 5, 'loss': 7.300241947174072}\n",
      "{'epoch': 1, 'batch': 6, 'loss': 7.6840291023254395}\n",
      "{'epoch': 1, 'batch': 7, 'loss': 7.503852844238281}\n",
      "{'epoch': 1, 'batch': 8, 'loss': 7.4531073570251465}\n",
      "{'epoch': 1, 'batch': 9, 'loss': 7.2963361740112305}\n",
      "{'epoch': 1, 'batch': 10, 'loss': 7.241095542907715}\n",
      "{'epoch': 1, 'batch': 11, 'loss': 7.132656097412109}\n",
      "{'epoch': 1, 'batch': 12, 'loss': 7.176458835601807}\n",
      "{'epoch': 1, 'batch': 13, 'loss': 7.210395812988281}\n",
      "{'epoch': 1, 'batch': 14, 'loss': 6.886795997619629}\n",
      "{'epoch': 1, 'batch': 15, 'loss': 6.9518208503723145}\n",
      "{'epoch': 1, 'batch': 16, 'loss': 6.725218296051025}\n",
      "{'epoch': 1, 'batch': 17, 'loss': 6.9110026359558105}\n",
      "{'epoch': 1, 'batch': 18, 'loss': 6.835825443267822}\n",
      "{'epoch': 1, 'batch': 19, 'loss': 6.927613258361816}\n",
      "{'epoch': 1, 'batch': 20, 'loss': 6.654507160186768}\n",
      "{'epoch': 1, 'batch': 21, 'loss': 7.038845062255859}\n",
      "{'epoch': 1, 'batch': 22, 'loss': 7.007198810577393}\n",
      "{'epoch': 1, 'batch': 23, 'loss': 7.109439849853516}\n",
      "{'epoch': 1, 'batch': 24, 'loss': 7.067408084869385}\n",
      "{'epoch': 1, 'batch': 25, 'loss': 6.814904689788818}\n",
      "{'epoch': 1, 'batch': 26, 'loss': 6.794761657714844}\n",
      "{'epoch': 1, 'batch': 27, 'loss': 6.82573938369751}\n",
      "{'epoch': 1, 'batch': 28, 'loss': 7.22029972076416}\n",
      "{'epoch': 1, 'batch': 29, 'loss': 7.311784267425537}\n",
      "{'epoch': 1, 'batch': 30, 'loss': 6.703984260559082}\n",
      "{'epoch': 1, 'batch': 31, 'loss': 6.647590160369873}\n",
      "{'epoch': 1, 'batch': 32, 'loss': 6.790643215179443}\n",
      "{'epoch': 1, 'batch': 33, 'loss': 6.973540306091309}\n",
      "{'epoch': 1, 'batch': 34, 'loss': 6.926758766174316}\n",
      "{'epoch': 1, 'batch': 35, 'loss': 7.208985805511475}\n",
      "{'epoch': 1, 'batch': 36, 'loss': 7.131623268127441}\n",
      "{'epoch': 1, 'batch': 37, 'loss': 6.927548408508301}\n",
      "{'epoch': 1, 'batch': 38, 'loss': 7.2216949462890625}\n",
      "{'epoch': 1, 'batch': 39, 'loss': 7.068878650665283}\n",
      "{'epoch': 1, 'batch': 40, 'loss': 7.302943706512451}\n",
      "{'epoch': 1, 'batch': 41, 'loss': 6.998339653015137}\n",
      "{'epoch': 1, 'batch': 42, 'loss': 7.255104064941406}\n",
      "{'epoch': 1, 'batch': 43, 'loss': 7.009260177612305}\n",
      "{'epoch': 1, 'batch': 44, 'loss': 6.909879207611084}\n",
      "{'epoch': 1, 'batch': 45, 'loss': 7.04985237121582}\n",
      "{'epoch': 1, 'batch': 46, 'loss': 7.212663650512695}\n",
      "{'epoch': 1, 'batch': 47, 'loss': 7.486170291900635}\n",
      "{'epoch': 1, 'batch': 48, 'loss': 6.963305473327637}\n",
      "{'epoch': 1, 'batch': 49, 'loss': 7.202205657958984}\n",
      "{'epoch': 1, 'batch': 50, 'loss': 7.402761459350586}\n",
      "{'epoch': 1, 'batch': 51, 'loss': 7.22956657409668}\n",
      "{'epoch': 1, 'batch': 52, 'loss': 6.854382514953613}\n",
      "{'epoch': 1, 'batch': 53, 'loss': 7.113745212554932}\n",
      "{'epoch': 1, 'batch': 54, 'loss': 7.01216983795166}\n",
      "{'epoch': 1, 'batch': 55, 'loss': 7.050948619842529}\n",
      "{'epoch': 1, 'batch': 56, 'loss': 7.12429141998291}\n",
      "{'epoch': 1, 'batch': 57, 'loss': 7.108189105987549}\n",
      "{'epoch': 1, 'batch': 58, 'loss': 7.07126522064209}\n",
      "{'epoch': 1, 'batch': 59, 'loss': 7.10981559753418}\n",
      "{'epoch': 1, 'batch': 60, 'loss': 7.031543731689453}\n",
      "{'epoch': 1, 'batch': 61, 'loss': 7.197856903076172}\n",
      "{'epoch': 1, 'batch': 62, 'loss': 7.167698383331299}\n",
      "{'epoch': 1, 'batch': 63, 'loss': 7.103442192077637}\n",
      "{'epoch': 1, 'batch': 64, 'loss': 7.210896968841553}\n",
      "{'epoch': 1, 'batch': 65, 'loss': 7.1362128257751465}\n",
      "{'epoch': 1, 'batch': 66, 'loss': 7.119466304779053}\n",
      "{'epoch': 1, 'batch': 67, 'loss': 6.937010765075684}\n",
      "{'epoch': 1, 'batch': 68, 'loss': 7.139062881469727}\n",
      "{'epoch': 1, 'batch': 69, 'loss': 6.883293628692627}\n",
      "{'epoch': 1, 'batch': 70, 'loss': 7.3149285316467285}\n",
      "{'epoch': 1, 'batch': 71, 'loss': 7.254060745239258}\n",
      "{'epoch': 1, 'batch': 72, 'loss': 7.163717269897461}\n",
      "{'epoch': 1, 'batch': 73, 'loss': 7.229952812194824}\n",
      "{'epoch': 1, 'batch': 74, 'loss': 7.2423601150512695}\n",
      "{'epoch': 1, 'batch': 75, 'loss': 7.384843349456787}\n",
      "{'epoch': 1, 'batch': 76, 'loss': 7.160934925079346}\n",
      "{'epoch': 1, 'batch': 77, 'loss': 7.408392429351807}\n",
      "{'epoch': 1, 'batch': 78, 'loss': 7.5304179191589355}\n",
      "{'epoch': 1, 'batch': 79, 'loss': 6.838548183441162}\n",
      "{'epoch': 1, 'batch': 80, 'loss': 7.126063823699951}\n",
      "{'epoch': 1, 'batch': 81, 'loss': 7.2889814376831055}\n",
      "{'epoch': 1, 'batch': 82, 'loss': 7.318271636962891}\n",
      "{'epoch': 1, 'batch': 83, 'loss': 7.356344223022461}\n",
      "{'epoch': 1, 'batch': 84, 'loss': 7.15444803237915}\n",
      "{'epoch': 1, 'batch': 85, 'loss': 7.360201358795166}\n",
      "{'epoch': 1, 'batch': 86, 'loss': 7.098724842071533}\n",
      "{'epoch': 1, 'batch': 87, 'loss': 7.215187072753906}\n",
      "{'epoch': 1, 'batch': 88, 'loss': 7.103367328643799}\n",
      "{'epoch': 1, 'batch': 89, 'loss': 7.208747863769531}\n",
      "{'epoch': 1, 'batch': 90, 'loss': 7.581358432769775}\n",
      "{'epoch': 1, 'batch': 91, 'loss': 7.078064918518066}\n",
      "{'epoch': 1, 'batch': 92, 'loss': 7.31221866607666}\n",
      "{'epoch': 1, 'batch': 93, 'loss': 6.886670112609863}\n",
      "{'epoch': 2, 'batch': 0, 'loss': 7.089966297149658}\n",
      "{'epoch': 2, 'batch': 1, 'loss': 7.084926605224609}\n",
      "{'epoch': 2, 'batch': 2, 'loss': 7.029390335083008}\n",
      "{'epoch': 2, 'batch': 3, 'loss': 7.247437953948975}\n",
      "{'epoch': 2, 'batch': 4, 'loss': 7.204516410827637}\n",
      "{'epoch': 2, 'batch': 5, 'loss': 7.2071614265441895}\n",
      "{'epoch': 2, 'batch': 6, 'loss': 7.583162307739258}\n",
      "{'epoch': 2, 'batch': 7, 'loss': 7.407183647155762}\n",
      "{'epoch': 2, 'batch': 8, 'loss': 7.350554466247559}\n",
      "{'epoch': 2, 'batch': 9, 'loss': 7.214256763458252}\n",
      "{'epoch': 2, 'batch': 10, 'loss': 7.1885809898376465}\n",
      "{'epoch': 2, 'batch': 11, 'loss': 7.112523555755615}\n",
      "{'epoch': 2, 'batch': 12, 'loss': 7.176910400390625}\n",
      "{'epoch': 2, 'batch': 13, 'loss': 7.25242280960083}\n",
      "{'epoch': 2, 'batch': 14, 'loss': 6.930156230926514}\n",
      "{'epoch': 2, 'batch': 15, 'loss': 7.008053779602051}\n",
      "{'epoch': 2, 'batch': 16, 'loss': 6.7849321365356445}\n",
      "{'epoch': 2, 'batch': 17, 'loss': 6.987873077392578}\n",
      "{'epoch': 2, 'batch': 18, 'loss': 6.886632919311523}\n",
      "{'epoch': 2, 'batch': 19, 'loss': 6.9987874031066895}\n",
      "{'epoch': 2, 'batch': 20, 'loss': 6.716617107391357}\n",
      "{'epoch': 2, 'batch': 21, 'loss': 7.116235733032227}\n",
      "{'epoch': 2, 'batch': 22, 'loss': 7.075624942779541}\n",
      "{'epoch': 2, 'batch': 23, 'loss': 7.174926280975342}\n",
      "{'epoch': 2, 'batch': 24, 'loss': 7.138513088226318}\n",
      "{'epoch': 2, 'batch': 25, 'loss': 6.866835594177246}\n",
      "{'epoch': 2, 'batch': 26, 'loss': 6.853855133056641}\n",
      "{'epoch': 2, 'batch': 27, 'loss': 6.864482879638672}\n",
      "{'epoch': 2, 'batch': 28, 'loss': 7.258415699005127}\n",
      "{'epoch': 2, 'batch': 29, 'loss': 7.3595356941223145}\n",
      "{'epoch': 2, 'batch': 30, 'loss': 6.7168450355529785}\n",
      "{'epoch': 2, 'batch': 31, 'loss': 6.6660966873168945}\n",
      "{'epoch': 2, 'batch': 32, 'loss': 6.809266090393066}\n",
      "{'epoch': 2, 'batch': 33, 'loss': 6.990145683288574}\n",
      "{'epoch': 2, 'batch': 34, 'loss': 6.926857948303223}\n",
      "{'epoch': 2, 'batch': 35, 'loss': 7.213930606842041}\n",
      "{'epoch': 2, 'batch': 36, 'loss': 7.123376846313477}\n",
      "{'epoch': 2, 'batch': 37, 'loss': 6.92347526550293}\n",
      "{'epoch': 2, 'batch': 38, 'loss': 7.226196765899658}\n",
      "{'epoch': 2, 'batch': 39, 'loss': 7.049646377563477}\n",
      "{'epoch': 2, 'batch': 40, 'loss': 7.278637886047363}\n",
      "{'epoch': 2, 'batch': 41, 'loss': 6.9857563972473145}\n",
      "{'epoch': 2, 'batch': 42, 'loss': 7.241739273071289}\n",
      "{'epoch': 2, 'batch': 43, 'loss': 6.962050437927246}\n",
      "{'epoch': 2, 'batch': 44, 'loss': 6.880893707275391}\n",
      "{'epoch': 2, 'batch': 45, 'loss': 6.994867324829102}\n",
      "{'epoch': 2, 'batch': 46, 'loss': 7.172619819641113}\n",
      "{'epoch': 2, 'batch': 47, 'loss': 7.462423801422119}\n",
      "{'epoch': 2, 'batch': 48, 'loss': 6.901618957519531}\n",
      "{'epoch': 2, 'batch': 49, 'loss': 7.150832176208496}\n",
      "{'epoch': 2, 'batch': 50, 'loss': 7.323951721191406}\n",
      "{'epoch': 2, 'batch': 51, 'loss': 7.184011936187744}\n",
      "{'epoch': 2, 'batch': 52, 'loss': 6.7820143699646}\n",
      "{'epoch': 2, 'batch': 53, 'loss': 7.05365514755249}\n",
      "{'epoch': 2, 'batch': 54, 'loss': 6.955982208251953}\n",
      "{'epoch': 2, 'batch': 55, 'loss': 6.978585243225098}\n",
      "{'epoch': 2, 'batch': 56, 'loss': 7.041214942932129}\n",
      "{'epoch': 2, 'batch': 57, 'loss': 7.003245830535889}\n",
      "{'epoch': 2, 'batch': 58, 'loss': 6.950708389282227}\n",
      "{'epoch': 2, 'batch': 59, 'loss': 7.0345988273620605}\n",
      "{'epoch': 2, 'batch': 60, 'loss': 6.9233880043029785}\n",
      "{'epoch': 2, 'batch': 61, 'loss': 7.094958305358887}\n",
      "{'epoch': 2, 'batch': 62, 'loss': 7.075750827789307}\n",
      "{'epoch': 2, 'batch': 63, 'loss': 6.992438793182373}\n",
      "{'epoch': 2, 'batch': 64, 'loss': 7.094058990478516}\n",
      "{'epoch': 2, 'batch': 65, 'loss': 6.9843926429748535}\n",
      "{'epoch': 2, 'batch': 66, 'loss': 6.9847307205200195}\n",
      "{'epoch': 2, 'batch': 67, 'loss': 6.821434020996094}\n",
      "{'epoch': 2, 'batch': 68, 'loss': 7.021931171417236}\n",
      "{'epoch': 2, 'batch': 69, 'loss': 6.742474555969238}\n",
      "{'epoch': 2, 'batch': 70, 'loss': 7.2182936668396}\n",
      "{'epoch': 2, 'batch': 71, 'loss': 7.132320404052734}\n",
      "{'epoch': 2, 'batch': 72, 'loss': 7.037576675415039}\n",
      "{'epoch': 2, 'batch': 73, 'loss': 7.090674877166748}\n",
      "{'epoch': 2, 'batch': 74, 'loss': 7.1106858253479}\n",
      "{'epoch': 2, 'batch': 75, 'loss': 7.194705009460449}\n",
      "{'epoch': 2, 'batch': 76, 'loss': 7.022653579711914}\n",
      "{'epoch': 2, 'batch': 77, 'loss': 7.278748035430908}\n",
      "{'epoch': 2, 'batch': 78, 'loss': 7.3530473709106445}\n",
      "{'epoch': 2, 'batch': 79, 'loss': 6.746016025543213}\n",
      "{'epoch': 2, 'batch': 80, 'loss': 6.936614513397217}\n",
      "{'epoch': 2, 'batch': 81, 'loss': 7.11956787109375}\n",
      "{'epoch': 2, 'batch': 82, 'loss': 7.147960662841797}\n",
      "{'epoch': 2, 'batch': 83, 'loss': 7.204591274261475}\n",
      "{'epoch': 2, 'batch': 84, 'loss': 7.031434535980225}\n",
      "{'epoch': 2, 'batch': 85, 'loss': 7.231248378753662}\n",
      "{'epoch': 2, 'batch': 86, 'loss': 6.9378252029418945}\n",
      "{'epoch': 2, 'batch': 87, 'loss': 7.063787937164307}\n",
      "{'epoch': 2, 'batch': 88, 'loss': 6.934635162353516}\n",
      "{'epoch': 2, 'batch': 89, 'loss': 7.010636806488037}\n",
      "{'epoch': 2, 'batch': 90, 'loss': 7.411681175231934}\n",
      "{'epoch': 2, 'batch': 91, 'loss': 6.926795959472656}\n",
      "{'epoch': 2, 'batch': 92, 'loss': 7.1417717933654785}\n",
      "{'epoch': 2, 'batch': 93, 'loss': 6.657069206237793}\n",
      "{'epoch': 3, 'batch': 0, 'loss': 6.9838337898254395}\n",
      "{'epoch': 3, 'batch': 1, 'loss': 6.933428764343262}\n",
      "{'epoch': 3, 'batch': 2, 'loss': 6.913674831390381}\n",
      "{'epoch': 3, 'batch': 3, 'loss': 7.128901481628418}\n",
      "{'epoch': 3, 'batch': 4, 'loss': 7.059162616729736}\n",
      "{'epoch': 3, 'batch': 5, 'loss': 7.056877613067627}\n",
      "{'epoch': 3, 'batch': 6, 'loss': 7.467238903045654}\n",
      "{'epoch': 3, 'batch': 7, 'loss': 7.278204441070557}\n",
      "{'epoch': 3, 'batch': 8, 'loss': 7.1742424964904785}\n",
      "{'epoch': 3, 'batch': 9, 'loss': 7.124338626861572}\n",
      "{'epoch': 3, 'batch': 10, 'loss': 7.133718490600586}\n",
      "{'epoch': 3, 'batch': 11, 'loss': 7.0466227531433105}\n",
      "{'epoch': 3, 'batch': 12, 'loss': 7.101423740386963}\n",
      "{'epoch': 3, 'batch': 13, 'loss': 7.215447425842285}\n",
      "{'epoch': 3, 'batch': 14, 'loss': 6.812309741973877}\n",
      "{'epoch': 3, 'batch': 15, 'loss': 6.936314105987549}\n",
      "{'epoch': 3, 'batch': 16, 'loss': 6.690738201141357}\n",
      "{'epoch': 3, 'batch': 17, 'loss': 6.9084906578063965}\n",
      "{'epoch': 3, 'batch': 18, 'loss': 6.784754276275635}\n",
      "{'epoch': 3, 'batch': 19, 'loss': 6.920594215393066}\n",
      "{'epoch': 3, 'batch': 20, 'loss': 6.582427501678467}\n",
      "{'epoch': 3, 'batch': 21, 'loss': 7.0442585945129395}\n",
      "{'epoch': 3, 'batch': 22, 'loss': 7.0214080810546875}\n",
      "{'epoch': 3, 'batch': 23, 'loss': 7.108919143676758}\n",
      "{'epoch': 3, 'batch': 24, 'loss': 7.049466133117676}\n",
      "{'epoch': 3, 'batch': 25, 'loss': 6.798872470855713}\n",
      "{'epoch': 3, 'batch': 26, 'loss': 6.730220794677734}\n",
      "{'epoch': 3, 'batch': 27, 'loss': 6.733623504638672}\n",
      "{'epoch': 3, 'batch': 28, 'loss': 7.1698737144470215}\n",
      "{'epoch': 3, 'batch': 29, 'loss': 7.265692710876465}\n",
      "{'epoch': 3, 'batch': 30, 'loss': 6.614378929138184}\n",
      "{'epoch': 3, 'batch': 31, 'loss': 6.539294719696045}\n",
      "{'epoch': 3, 'batch': 32, 'loss': 6.668058395385742}\n",
      "{'epoch': 3, 'batch': 33, 'loss': 6.909589767456055}\n",
      "{'epoch': 3, 'batch': 34, 'loss': 6.854182243347168}\n",
      "{'epoch': 3, 'batch': 35, 'loss': 7.084173679351807}\n",
      "{'epoch': 3, 'batch': 36, 'loss': 7.024639129638672}\n",
      "{'epoch': 3, 'batch': 37, 'loss': 6.815554618835449}\n",
      "{'epoch': 3, 'batch': 38, 'loss': 7.132875919342041}\n",
      "{'epoch': 3, 'batch': 39, 'loss': 6.923957824707031}\n",
      "{'epoch': 3, 'batch': 40, 'loss': 7.165770530700684}\n",
      "{'epoch': 3, 'batch': 41, 'loss': 6.848984241485596}\n",
      "{'epoch': 3, 'batch': 42, 'loss': 7.121699333190918}\n",
      "{'epoch': 3, 'batch': 43, 'loss': 6.839136123657227}\n",
      "{'epoch': 3, 'batch': 44, 'loss': 6.790666103363037}\n",
      "{'epoch': 3, 'batch': 45, 'loss': 6.876991271972656}\n",
      "{'epoch': 3, 'batch': 46, 'loss': 7.045253276824951}\n",
      "{'epoch': 3, 'batch': 47, 'loss': 7.377191066741943}\n",
      "{'epoch': 3, 'batch': 48, 'loss': 6.730413913726807}\n",
      "{'epoch': 3, 'batch': 49, 'loss': 7.049117565155029}\n",
      "{'epoch': 3, 'batch': 50, 'loss': 7.192729949951172}\n",
      "{'epoch': 3, 'batch': 51, 'loss': 7.100027561187744}\n",
      "{'epoch': 3, 'batch': 52, 'loss': 6.600050449371338}\n",
      "{'epoch': 3, 'batch': 53, 'loss': 6.948943614959717}\n",
      "{'epoch': 3, 'batch': 54, 'loss': 6.828222751617432}\n",
      "{'epoch': 3, 'batch': 55, 'loss': 6.861222267150879}\n",
      "{'epoch': 3, 'batch': 56, 'loss': 6.915548801422119}\n",
      "{'epoch': 3, 'batch': 57, 'loss': 6.86706018447876}\n",
      "{'epoch': 3, 'batch': 58, 'loss': 6.793929100036621}\n",
      "{'epoch': 3, 'batch': 59, 'loss': 6.929959297180176}\n",
      "{'epoch': 3, 'batch': 60, 'loss': 6.792367458343506}\n",
      "{'epoch': 3, 'batch': 61, 'loss': 6.96994161605835}\n",
      "{'epoch': 3, 'batch': 62, 'loss': 6.96760892868042}\n",
      "{'epoch': 3, 'batch': 63, 'loss': 6.894853115081787}\n",
      "{'epoch': 3, 'batch': 64, 'loss': 6.959008693695068}\n",
      "{'epoch': 3, 'batch': 65, 'loss': 6.864780426025391}\n",
      "{'epoch': 3, 'batch': 66, 'loss': 6.856590270996094}\n",
      "{'epoch': 3, 'batch': 67, 'loss': 6.678282260894775}\n",
      "{'epoch': 3, 'batch': 68, 'loss': 6.896597385406494}\n",
      "{'epoch': 3, 'batch': 69, 'loss': 6.605999946594238}\n",
      "{'epoch': 3, 'batch': 70, 'loss': 7.103590965270996}\n",
      "{'epoch': 3, 'batch': 71, 'loss': 6.9789652824401855}\n",
      "{'epoch': 3, 'batch': 72, 'loss': 6.9118547439575195}\n",
      "{'epoch': 3, 'batch': 73, 'loss': 6.9676337242126465}\n",
      "{'epoch': 3, 'batch': 74, 'loss': 6.9669013023376465}\n",
      "{'epoch': 3, 'batch': 75, 'loss': 7.041194438934326}\n",
      "{'epoch': 3, 'batch': 76, 'loss': 6.89213752746582}\n",
      "{'epoch': 3, 'batch': 77, 'loss': 7.164846420288086}\n",
      "{'epoch': 3, 'batch': 78, 'loss': 7.21368932723999}\n",
      "{'epoch': 3, 'batch': 79, 'loss': 6.602351665496826}\n",
      "{'epoch': 3, 'batch': 80, 'loss': 6.765843868255615}\n",
      "{'epoch': 3, 'batch': 81, 'loss': 6.994997978210449}\n",
      "{'epoch': 3, 'batch': 82, 'loss': 6.961528301239014}\n",
      "{'epoch': 3, 'batch': 83, 'loss': 7.05564022064209}\n",
      "{'epoch': 3, 'batch': 84, 'loss': 6.924015998840332}\n",
      "{'epoch': 3, 'batch': 85, 'loss': 7.087888717651367}\n",
      "{'epoch': 3, 'batch': 86, 'loss': 6.786713600158691}\n",
      "{'epoch': 3, 'batch': 87, 'loss': 6.915211200714111}\n",
      "{'epoch': 3, 'batch': 88, 'loss': 6.779315948486328}\n",
      "{'epoch': 3, 'batch': 89, 'loss': 6.857253551483154}\n",
      "{'epoch': 3, 'batch': 90, 'loss': 7.286086082458496}\n",
      "{'epoch': 3, 'batch': 91, 'loss': 6.784373760223389}\n",
      "{'epoch': 3, 'batch': 92, 'loss': 6.998790740966797}\n",
      "{'epoch': 3, 'batch': 93, 'loss': 6.460437774658203}\n",
      "{'epoch': 4, 'batch': 0, 'loss': 6.861050128936768}\n",
      "{'epoch': 4, 'batch': 1, 'loss': 6.814746379852295}\n",
      "{'epoch': 4, 'batch': 2, 'loss': 6.79984712600708}\n",
      "{'epoch': 4, 'batch': 3, 'loss': 6.991436004638672}\n",
      "{'epoch': 4, 'batch': 4, 'loss': 6.91338586807251}\n",
      "{'epoch': 4, 'batch': 5, 'loss': 6.89141845703125}\n",
      "{'epoch': 4, 'batch': 6, 'loss': 7.35551643371582}\n",
      "{'epoch': 4, 'batch': 7, 'loss': 7.156027793884277}\n",
      "{'epoch': 4, 'batch': 8, 'loss': 7.041338920593262}\n",
      "{'epoch': 4, 'batch': 9, 'loss': 6.9921770095825195}\n",
      "{'epoch': 4, 'batch': 10, 'loss': 7.028111934661865}\n",
      "{'epoch': 4, 'batch': 11, 'loss': 6.916262149810791}\n",
      "{'epoch': 4, 'batch': 12, 'loss': 7.004990100860596}\n",
      "{'epoch': 4, 'batch': 13, 'loss': 7.131438732147217}\n",
      "{'epoch': 4, 'batch': 14, 'loss': 6.703111171722412}\n",
      "{'epoch': 4, 'batch': 15, 'loss': 6.8521599769592285}\n",
      "{'epoch': 4, 'batch': 16, 'loss': 6.595098495483398}\n",
      "{'epoch': 4, 'batch': 17, 'loss': 6.809643268585205}\n",
      "{'epoch': 4, 'batch': 18, 'loss': 6.676204681396484}\n",
      "{'epoch': 4, 'batch': 19, 'loss': 6.829768657684326}\n",
      "{'epoch': 4, 'batch': 20, 'loss': 6.466294765472412}\n",
      "{'epoch': 4, 'batch': 21, 'loss': 6.948958873748779}\n",
      "{'epoch': 4, 'batch': 22, 'loss': 6.949025630950928}\n",
      "{'epoch': 4, 'batch': 23, 'loss': 7.039854526519775}\n",
      "{'epoch': 4, 'batch': 24, 'loss': 6.988555908203125}\n",
      "{'epoch': 4, 'batch': 25, 'loss': 6.726426601409912}\n",
      "{'epoch': 4, 'batch': 26, 'loss': 6.622804641723633}\n",
      "{'epoch': 4, 'batch': 27, 'loss': 6.629263401031494}\n",
      "{'epoch': 4, 'batch': 28, 'loss': 7.11144495010376}\n",
      "{'epoch': 4, 'batch': 29, 'loss': 7.195610523223877}\n",
      "{'epoch': 4, 'batch': 30, 'loss': 6.5125017166137695}\n",
      "{'epoch': 4, 'batch': 31, 'loss': 6.429241180419922}\n",
      "{'epoch': 4, 'batch': 32, 'loss': 6.563870906829834}\n",
      "{'epoch': 4, 'batch': 33, 'loss': 6.82863187789917}\n",
      "{'epoch': 4, 'batch': 34, 'loss': 6.769306659698486}\n",
      "{'epoch': 4, 'batch': 35, 'loss': 6.9636149406433105}\n",
      "{'epoch': 4, 'batch': 36, 'loss': 6.903685092926025}\n",
      "{'epoch': 4, 'batch': 37, 'loss': 6.7415690422058105}\n",
      "{'epoch': 4, 'batch': 38, 'loss': 7.066075801849365}\n",
      "{'epoch': 4, 'batch': 39, 'loss': 6.827782154083252}\n",
      "{'epoch': 4, 'batch': 40, 'loss': 7.078165531158447}\n",
      "{'epoch': 4, 'batch': 41, 'loss': 6.729775428771973}\n",
      "{'epoch': 4, 'batch': 42, 'loss': 7.043339252471924}\n",
      "{'epoch': 4, 'batch': 43, 'loss': 6.7270307540893555}\n",
      "{'epoch': 4, 'batch': 44, 'loss': 6.699827671051025}\n",
      "{'epoch': 4, 'batch': 45, 'loss': 6.777498245239258}\n",
      "{'epoch': 4, 'batch': 46, 'loss': 6.940205097198486}\n",
      "{'epoch': 4, 'batch': 47, 'loss': 7.313363552093506}\n",
      "{'epoch': 4, 'batch': 48, 'loss': 6.599483489990234}\n",
      "{'epoch': 4, 'batch': 49, 'loss': 6.955868721008301}\n",
      "{'epoch': 4, 'batch': 50, 'loss': 7.078737258911133}\n",
      "{'epoch': 4, 'batch': 51, 'loss': 7.003372669219971}\n",
      "{'epoch': 4, 'batch': 52, 'loss': 6.431742191314697}\n",
      "{'epoch': 4, 'batch': 53, 'loss': 6.8226237297058105}\n",
      "{'epoch': 4, 'batch': 54, 'loss': 6.703503608703613}\n",
      "{'epoch': 4, 'batch': 55, 'loss': 6.735823154449463}\n",
      "{'epoch': 4, 'batch': 56, 'loss': 6.79088020324707}\n",
      "{'epoch': 4, 'batch': 57, 'loss': 6.735219955444336}\n",
      "{'epoch': 4, 'batch': 58, 'loss': 6.6477508544921875}\n",
      "{'epoch': 4, 'batch': 59, 'loss': 6.795815944671631}\n",
      "{'epoch': 4, 'batch': 60, 'loss': 6.660121917724609}\n",
      "{'epoch': 4, 'batch': 61, 'loss': 6.855954647064209}\n",
      "{'epoch': 4, 'batch': 62, 'loss': 6.844247817993164}\n",
      "{'epoch': 4, 'batch': 63, 'loss': 6.757978916168213}\n",
      "{'epoch': 4, 'batch': 64, 'loss': 6.776371002197266}\n",
      "{'epoch': 4, 'batch': 65, 'loss': 6.740676403045654}\n",
      "{'epoch': 4, 'batch': 66, 'loss': 6.72817850112915}\n",
      "{'epoch': 4, 'batch': 67, 'loss': 6.540482521057129}\n",
      "{'epoch': 4, 'batch': 68, 'loss': 6.75624942779541}\n",
      "{'epoch': 4, 'batch': 69, 'loss': 6.470463752746582}\n",
      "{'epoch': 4, 'batch': 70, 'loss': 6.9901604652404785}\n",
      "{'epoch': 4, 'batch': 71, 'loss': 6.833182334899902}\n",
      "{'epoch': 4, 'batch': 72, 'loss': 6.780080318450928}\n",
      "{'epoch': 4, 'batch': 73, 'loss': 6.823877811431885}\n",
      "{'epoch': 4, 'batch': 74, 'loss': 6.8300018310546875}\n",
      "{'epoch': 4, 'batch': 75, 'loss': 6.903414249420166}\n",
      "{'epoch': 4, 'batch': 76, 'loss': 6.772643089294434}\n",
      "{'epoch': 4, 'batch': 77, 'loss': 7.061444282531738}\n",
      "{'epoch': 4, 'batch': 78, 'loss': 7.074599742889404}\n",
      "{'epoch': 4, 'batch': 79, 'loss': 6.465567111968994}\n",
      "{'epoch': 4, 'batch': 80, 'loss': 6.649296760559082}\n",
      "{'epoch': 4, 'batch': 81, 'loss': 6.8669962882995605}\n",
      "{'epoch': 4, 'batch': 82, 'loss': 6.803595542907715}\n",
      "{'epoch': 4, 'batch': 83, 'loss': 6.956386566162109}\n",
      "{'epoch': 4, 'batch': 84, 'loss': 6.841845512390137}\n",
      "{'epoch': 4, 'batch': 85, 'loss': 6.985584259033203}\n",
      "{'epoch': 4, 'batch': 86, 'loss': 6.676963806152344}\n",
      "{'epoch': 4, 'batch': 87, 'loss': 6.800047397613525}\n",
      "{'epoch': 4, 'batch': 88, 'loss': 6.657599925994873}\n",
      "{'epoch': 4, 'batch': 89, 'loss': 6.744206428527832}\n",
      "{'epoch': 4, 'batch': 90, 'loss': 7.211584091186523}\n",
      "{'epoch': 4, 'batch': 91, 'loss': 6.682004451751709}\n",
      "{'epoch': 4, 'batch': 92, 'loss': 6.9141926765441895}\n",
      "{'epoch': 4, 'batch': 93, 'loss': 6.316138744354248}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "train(dataset, model, batch_size=256,sequence_length=4,max_epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Knock', 'knock.', 'Whos', 'there?', 'strip?', 'work?', 'type', 'a', 'chickens?', 'like', \"can't\", 'college', 'TV', 'LED', 'What', 'you', 'a', 'corner.', 'Bros.', 'rows', 'vote', 'an', 'major', 'to', 'a', 'Aonther', 'named', 'E,', 'I', 'native', 'I', 'Karl', 'do', 'did', 'four', 'be', 'dino', 'want', 'be', 'the', '-so', '\"Kelp', 'were', 'felt', 'gold', 'disappointed.', 'if', 'the', 'tissues?', 'and', \"What's\", 'would', 'Vulgar', 'who?\\'\"', 'from', 'with', 'teeth!', 'moving', 'woof,', 'brand', 'How', 'do', 'you', 'the', '/r/cleanjokes', 'instead', 'had', 'fitness', 'many', 'sentence', 'I', 'would', 'hallucination?', 'Vector', 'She', 'Chucky.', 'think', 'Thai', 'programming', 'elephant', 'more', 'funny.', 'a', 'plant', 'Who', 'doors?', 'SC', 'Farts.', 'two', 'up', 'saying,', 'like?\"', 'What', 'do', 'cow', 'What', 'is', 'there?', \"Kellog's\", 'a', '\"Stay', 'dogs', 'remembered', 'their']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_count 1    self.capacity  2\n",
      "0  ------------------------------------------------\n",
      "epoch x\n",
      "[256, 4]\n",
      "epoch y\n",
      "[256, 4]\n",
      "forward x\n",
      "[256, 4]\n",
      "forward embed\n",
      "[256, 4, 128]\n",
      "forward output\n",
      "[256, 4, 128]\n",
      "forward state\n",
      "(tensor([[[ 0.0896, -0.0261,  0.0599,  ..., -0.1363,  0.3841,  0.0164],\n",
      "         [ 0.1657, -0.1122,  0.0972,  ...,  0.0735,  0.0454,  0.0112],\n",
      "         [ 0.1641, -0.0294,  0.1691,  ..., -0.1587,  0.0311, -0.1640],\n",
      "         [ 0.0763, -0.0298,  0.1172,  ..., -0.2956,  0.1862, -0.0689]],\n",
      "\n",
      "        [[ 0.0805,  0.0146,  0.0583,  ...,  0.0585, -0.0125,  0.0429],\n",
      "         [ 0.0648,  0.0095,  0.0372,  ...,  0.0621, -0.0077,  0.0078],\n",
      "         [ 0.0726, -0.0410,  0.0234,  ...,  0.0719, -0.0014, -0.0121],\n",
      "         [ 0.0669, -0.0242,  0.0345,  ...,  0.0983,  0.0069,  0.0097]],\n",
      "\n",
      "        [[-0.0025,  0.0107,  0.0083,  ..., -0.0345,  0.0055, -0.0223],\n",
      "         [-0.0178,  0.0065,  0.0159,  ..., -0.0346,  0.0046, -0.0082],\n",
      "         [-0.0117,  0.0136,  0.0143,  ..., -0.0329, -0.0005, -0.0131],\n",
      "         [-0.0322,  0.0133,  0.0077,  ..., -0.0328,  0.0094, -0.0110]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.1175, -0.0663,  0.1796,  ..., -0.2752,  0.7639,  0.0707],\n",
      "         [ 0.3104, -0.2369,  0.2599,  ...,  0.2072,  0.0946,  0.0216],\n",
      "         [ 0.2448, -0.0496,  0.2743,  ..., -0.3259,  0.0527, -0.2679],\n",
      "         [ 0.1591, -0.0971,  0.3025,  ..., -0.6293,  0.5362, -0.1361]],\n",
      "\n",
      "        [[ 0.1550,  0.0280,  0.1294,  ...,  0.1248, -0.0240,  0.0859],\n",
      "         [ 0.1290,  0.0196,  0.0773,  ...,  0.1365, -0.0167,  0.0161],\n",
      "         [ 0.1531, -0.0773,  0.0500,  ...,  0.1540, -0.0030, -0.0226],\n",
      "         [ 0.1421, -0.0444,  0.0733,  ...,  0.1986,  0.0140,  0.0196]],\n",
      "\n",
      "        [[-0.0051,  0.0211,  0.0160,  ..., -0.0650,  0.0110, -0.0436],\n",
      "         [-0.0356,  0.0131,  0.0307,  ..., -0.0662,  0.0093, -0.0162],\n",
      "         [-0.0235,  0.0274,  0.0274,  ..., -0.0624, -0.0011, -0.0256],\n",
      "         [-0.0648,  0.0267,  0.0149,  ..., -0.0627,  0.0187, -0.0217]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "forward logits\n",
      "[256, 4, 6925]\n",
      "model y_pred\n",
      "[256, 4, 6925]\n",
      "model state_h\n",
      "[3, 4, 128]\n",
      "model state_c\n",
      "[3, 4, 128]\n",
      "model loss\n",
      "[]\n",
      "predict x\n",
      "[1, 4]\n",
      "predict y_pred\n",
      "[1, 4, 6925]\n",
      "predict state_h\n",
      "[3, 4, 128]\n",
      "predict state_c\n",
      "[3, 4, 128]\n",
      "predict last_word_logits\n",
      "[6925]\n",
      "predict p\n",
      "[0.00031003 0.00029482 0.00029316 ... 0.00014308 0.00012737 0.00012531]\n",
      "predict word_index\n",
      "3582\n",
      "predict dataset.index_to_word[word_index]\n",
      "strip?\n"
     ]
    }
   ],
   "source": [
    "step_logger.get_default_summary(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary_count 1    self.capacity  2\n",
      "0  ------------------------------------------------\n",
      "epoch x\n",
      "[256, 4]\n",
      "tensor([[   2,    8,    0,  248],\n",
      "        [   8,    0,  248,   20],\n",
      "        [   0,  248,   20,    4],\n",
      "        ...,\n",
      "        [ 105, 1959,   32,    4],\n",
      "        [1959,   32,    4,   20],\n",
      "        [  32,    4,   20,   25]])\n",
      "epoch y\n",
      "[256, 4]\n",
      "tensor([[   8,    0,  248,   20],\n",
      "        [   0,  248,   20,    4],\n",
      "        [ 248,   20,    4,    0],\n",
      "        ...,\n",
      "        [1959,   32,    4,   20],\n",
      "        [  32,    4,   20,   25],\n",
      "        [   4,   20,   25, 1960]])\n",
      "forward x\n",
      "[256, 4]\n",
      "tensor([[   2,    8,    0,  248],\n",
      "        [   8,    0,  248,   20],\n",
      "        [   0,  248,   20,    4],\n",
      "        ...,\n",
      "        [ 105, 1959,   32,    4],\n",
      "        [1959,   32,    4,   20],\n",
      "        [  32,    4,   20,   25]])\n",
      "forward embed\n",
      "[256, 4, 128]\n",
      "tensor([[[-0.7877, -1.1969, -0.6383,  ...,  0.6313,  0.0760,  0.2087],\n",
      "         [-0.1420,  0.3932,  0.4376,  ..., -0.5294, -0.7462,  0.3916],\n",
      "         [-0.7515, -0.5439, -0.3029,  ...,  0.0630, -1.7748, -0.2703],\n",
      "         [-0.2337, -0.0365, -1.4209,  ..., -0.1238,  1.8884,  0.7283]],\n",
      "\n",
      "        [[-0.1420,  0.3932,  0.4376,  ..., -0.5294, -0.7462,  0.3916],\n",
      "         [-0.7515, -0.5439, -0.3029,  ...,  0.0630, -1.7748, -0.2703],\n",
      "         [-0.2337, -0.0365, -1.4209,  ..., -0.1238,  1.8884,  0.7283],\n",
      "         [ 0.5635,  0.8348, -0.1917,  ...,  0.8587, -0.0102,  1.5597]],\n",
      "\n",
      "        [[-0.7515, -0.5439, -0.3029,  ...,  0.0630, -1.7748, -0.2703],\n",
      "         [-0.2337, -0.0365, -1.4209,  ..., -0.1238,  1.8884,  0.7283],\n",
      "         [ 0.5635,  0.8348, -0.1917,  ...,  0.8587, -0.0102,  1.5597],\n",
      "         [-0.5161, -0.3011, -0.2559,  ..., -2.2655, -0.7681,  0.1033]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1164, -1.0045, -1.1237,  ...,  0.0085, -0.7967,  0.4711],\n",
      "         [-0.6866,  0.5789, -0.4095,  ...,  1.3285, -1.1450, -0.6209],\n",
      "         [ 0.0928, -0.1005, -0.3995,  ..., -0.3171,  0.2533,  0.7204],\n",
      "         [-0.5161, -0.3011, -0.2559,  ..., -2.2655, -0.7681,  0.1033]],\n",
      "\n",
      "        [[-0.6866,  0.5789, -0.4095,  ...,  1.3285, -1.1450, -0.6209],\n",
      "         [ 0.0928, -0.1005, -0.3995,  ..., -0.3171,  0.2533,  0.7204],\n",
      "         [-0.5161, -0.3011, -0.2559,  ..., -2.2655, -0.7681,  0.1033],\n",
      "         [ 0.5635,  0.8348, -0.1917,  ...,  0.8587, -0.0102,  1.5597]],\n",
      "\n",
      "        [[ 0.0928, -0.1005, -0.3995,  ..., -0.3171,  0.2533,  0.7204],\n",
      "         [-0.5161, -0.3011, -0.2559,  ..., -2.2655, -0.7681,  0.1033],\n",
      "         [ 0.5635,  0.8348, -0.1917,  ...,  0.8587, -0.0102,  1.5597],\n",
      "         [ 1.8504, -1.9539,  1.6191,  ..., -0.5145, -0.2382, -0.5726]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "forward output\n",
      "[256, 4, 128]\n",
      "tensor([[[-4.4350e-03,  2.1834e-03,  9.7556e-03,  ..., -1.2263e-03,\n",
      "           1.7759e-03, -7.8853e-04],\n",
      "         [-2.1905e-03, -1.8430e-03,  7.9246e-03,  ..., -6.6843e-03,\n",
      "           1.6814e-03, -2.3900e-03],\n",
      "         [-6.2384e-03, -1.8588e-03,  1.0882e-02,  ..., -6.6689e-03,\n",
      "           6.8507e-03,  2.0315e-03],\n",
      "         [-5.0544e-03,  8.8827e-05,  3.4621e-03,  ..., -5.6045e-04,\n",
      "          -1.1734e-03,  1.9256e-03]],\n",
      "\n",
      "        [[-1.5750e-02,  6.3982e-03,  1.2696e-02,  ..., -2.0433e-03,\n",
      "          -1.0300e-03, -2.5266e-03],\n",
      "         [-1.5599e-02, -1.8896e-03,  1.0133e-02,  ..., -1.0185e-02,\n",
      "           3.9906e-03, -8.4261e-03],\n",
      "         [-1.4123e-02, -5.2513e-03,  1.0192e-02,  ..., -8.5429e-03,\n",
      "           2.8316e-03, -2.5537e-03],\n",
      "         [-1.5151e-02,  8.2610e-04,  9.3285e-03,  ..., -7.8609e-03,\n",
      "          -9.3434e-04, -4.4808e-04]],\n",
      "\n",
      "        [[-2.3502e-02,  7.2002e-03,  1.2648e-02,  ..., -6.8506e-03,\n",
      "          -1.1767e-03, -3.2893e-03],\n",
      "         [-1.9012e-02, -2.2990e-03,  1.0334e-02,  ..., -9.8030e-03,\n",
      "          -6.3891e-04, -1.2722e-02],\n",
      "         [-1.4084e-02,  1.0846e-03,  9.0183e-03,  ..., -7.0458e-03,\n",
      "           1.0960e-03, -8.3151e-04],\n",
      "         [-1.5836e-02,  5.1266e-03,  2.8856e-03,  ..., -1.3133e-02,\n",
      "          -1.7806e-03, -1.9542e-03]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.2714e-03,  1.4622e-02,  3.4709e-03,  ..., -3.3677e-02,\n",
      "          -5.8480e-03, -3.3489e-02],\n",
      "         [-1.8999e-02,  3.6829e-03,  2.0922e-02,  ..., -3.8087e-02,\n",
      "          -1.1878e-02, -3.2194e-02],\n",
      "         [-6.4553e-03,  1.8511e-02,  1.5381e-02,  ..., -3.1412e-02,\n",
      "          -9.3142e-03, -2.8281e-02],\n",
      "         [-1.0744e-02,  1.2574e-02,  6.4131e-03,  ..., -2.7205e-02,\n",
      "           8.6692e-03, -2.3860e-02]],\n",
      "\n",
      "        [[-1.7616e-03,  1.1852e-02,  9.4635e-03,  ..., -3.8091e-02,\n",
      "           5.4812e-03, -3.8593e-02],\n",
      "         [-1.8465e-02,  2.9578e-03,  1.8407e-02,  ..., -3.8188e-02,\n",
      "           2.3807e-03, -2.1804e-02],\n",
      "         [-1.2061e-02,  1.8147e-02,  1.0256e-02,  ..., -3.7000e-02,\n",
      "          -3.5340e-03, -1.4948e-02],\n",
      "         [-2.1123e-02,  1.1015e-02,  1.0483e-02,  ..., -3.0942e-02,\n",
      "           5.7798e-03, -1.6590e-02]],\n",
      "\n",
      "        [[-2.4949e-03,  1.0700e-02,  8.2987e-03,  ..., -3.4534e-02,\n",
      "           5.4580e-03, -2.2257e-02],\n",
      "         [-1.7770e-02,  6.5412e-03,  1.5934e-02,  ..., -3.4640e-02,\n",
      "           4.6315e-03, -8.1739e-03],\n",
      "         [-1.1726e-02,  1.3609e-02,  1.4291e-02,  ..., -3.2877e-02,\n",
      "          -5.3634e-04, -1.3051e-02],\n",
      "         [-3.2214e-02,  1.3322e-02,  7.6629e-03,  ..., -3.2775e-02,\n",
      "           9.3581e-03, -1.0971e-02]]], grad_fn=<StackBackward>)\n",
      "forward state\n",
      "(tensor([[[ 0.0896, -0.0261,  0.0599,  ..., -0.1363,  0.3841,  0.0164],\n",
      "         [ 0.1657, -0.1122,  0.0972,  ...,  0.0735,  0.0454,  0.0112],\n",
      "         [ 0.1641, -0.0294,  0.1691,  ..., -0.1587,  0.0311, -0.1640],\n",
      "         [ 0.0763, -0.0298,  0.1172,  ..., -0.2956,  0.1862, -0.0689]],\n",
      "\n",
      "        [[ 0.0805,  0.0146,  0.0583,  ...,  0.0585, -0.0125,  0.0429],\n",
      "         [ 0.0648,  0.0095,  0.0372,  ...,  0.0621, -0.0077,  0.0078],\n",
      "         [ 0.0726, -0.0410,  0.0234,  ...,  0.0719, -0.0014, -0.0121],\n",
      "         [ 0.0669, -0.0242,  0.0345,  ...,  0.0983,  0.0069,  0.0097]],\n",
      "\n",
      "        [[-0.0025,  0.0107,  0.0083,  ..., -0.0345,  0.0055, -0.0223],\n",
      "         [-0.0178,  0.0065,  0.0159,  ..., -0.0346,  0.0046, -0.0082],\n",
      "         [-0.0117,  0.0136,  0.0143,  ..., -0.0329, -0.0005, -0.0131],\n",
      "         [-0.0322,  0.0133,  0.0077,  ..., -0.0328,  0.0094, -0.0110]]],\n",
      "       grad_fn=<StackBackward>), tensor([[[ 0.1175, -0.0663,  0.1796,  ..., -0.2752,  0.7639,  0.0707],\n",
      "         [ 0.3104, -0.2369,  0.2599,  ...,  0.2072,  0.0946,  0.0216],\n",
      "         [ 0.2448, -0.0496,  0.2743,  ..., -0.3259,  0.0527, -0.2679],\n",
      "         [ 0.1591, -0.0971,  0.3025,  ..., -0.6293,  0.5362, -0.1361]],\n",
      "\n",
      "        [[ 0.1550,  0.0280,  0.1294,  ...,  0.1248, -0.0240,  0.0859],\n",
      "         [ 0.1290,  0.0196,  0.0773,  ...,  0.1365, -0.0167,  0.0161],\n",
      "         [ 0.1531, -0.0773,  0.0500,  ...,  0.1540, -0.0030, -0.0226],\n",
      "         [ 0.1421, -0.0444,  0.0733,  ...,  0.1986,  0.0140,  0.0196]],\n",
      "\n",
      "        [[-0.0051,  0.0211,  0.0160,  ..., -0.0650,  0.0110, -0.0436],\n",
      "         [-0.0356,  0.0131,  0.0307,  ..., -0.0662,  0.0093, -0.0162],\n",
      "         [-0.0235,  0.0274,  0.0274,  ..., -0.0624, -0.0011, -0.0256],\n",
      "         [-0.0648,  0.0267,  0.0149,  ..., -0.0627,  0.0187, -0.0217]]],\n",
      "       grad_fn=<StackBackward>))\n",
      "forward logits\n",
      "[256, 4, 6925]\n",
      "tensor([[[-0.0356, -0.0803, -0.0490,  ...,  0.0257, -0.0473,  0.0553],\n",
      "         [-0.0355, -0.0783, -0.0465,  ...,  0.0255, -0.0453,  0.0544],\n",
      "         [-0.0332, -0.0848, -0.0525,  ...,  0.0232, -0.0459,  0.0571],\n",
      "         [-0.0321, -0.0800, -0.0463,  ...,  0.0269, -0.0446,  0.0557]],\n",
      "\n",
      "        [[-0.0463, -0.0756, -0.0462,  ...,  0.0304, -0.0503,  0.0571],\n",
      "         [-0.0447, -0.0776, -0.0480,  ...,  0.0298, -0.0448,  0.0577],\n",
      "         [-0.0411, -0.0837, -0.0524,  ...,  0.0259, -0.0453,  0.0565],\n",
      "         [-0.0396, -0.0798, -0.0450,  ...,  0.0307, -0.0467,  0.0581]],\n",
      "\n",
      "        [[-0.0458, -0.0748, -0.0441,  ...,  0.0343, -0.0503,  0.0588],\n",
      "         [-0.0494, -0.0734, -0.0462,  ...,  0.0324, -0.0451,  0.0562],\n",
      "         [-0.0447, -0.0824, -0.0458,  ...,  0.0313, -0.0446,  0.0560],\n",
      "         [-0.0454, -0.0791, -0.0437,  ...,  0.0353, -0.0425,  0.0575]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0507, -0.0797, -0.0365,  ...,  0.0388, -0.0495,  0.0572],\n",
      "         [-0.0477, -0.0832, -0.0366,  ...,  0.0377, -0.0491,  0.0577],\n",
      "         [-0.0459, -0.0696, -0.0372,  ...,  0.0420, -0.0608,  0.0584],\n",
      "         [-0.0498, -0.0719, -0.0408,  ...,  0.0479, -0.0586,  0.0609]],\n",
      "\n",
      "        [[-0.0506, -0.0811, -0.0360,  ...,  0.0396, -0.0524,  0.0588],\n",
      "         [-0.0503, -0.0825, -0.0392,  ...,  0.0428, -0.0495,  0.0609],\n",
      "         [-0.0442, -0.0725, -0.0373,  ...,  0.0507, -0.0581,  0.0608],\n",
      "         [-0.0521, -0.0760, -0.0450,  ...,  0.0485, -0.0584,  0.0614]],\n",
      "\n",
      "        [[-0.0477, -0.0803, -0.0417,  ...,  0.0434, -0.0528,  0.0617],\n",
      "         [-0.0485, -0.0807, -0.0435,  ...,  0.0459, -0.0489,  0.0622],\n",
      "         [-0.0458, -0.0736, -0.0361,  ...,  0.0512, -0.0564,  0.0555],\n",
      "         [-0.0463, -0.0740, -0.0490,  ...,  0.0441, -0.0519,  0.0578]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "model y_pred\n",
      "[256, 4, 6925]\n",
      "tensor([[[-0.0356, -0.0803, -0.0490,  ...,  0.0257, -0.0473,  0.0553],\n",
      "         [-0.0355, -0.0783, -0.0465,  ...,  0.0255, -0.0453,  0.0544],\n",
      "         [-0.0332, -0.0848, -0.0525,  ...,  0.0232, -0.0459,  0.0571],\n",
      "         [-0.0321, -0.0800, -0.0463,  ...,  0.0269, -0.0446,  0.0557]],\n",
      "\n",
      "        [[-0.0463, -0.0756, -0.0462,  ...,  0.0304, -0.0503,  0.0571],\n",
      "         [-0.0447, -0.0776, -0.0480,  ...,  0.0298, -0.0448,  0.0577],\n",
      "         [-0.0411, -0.0837, -0.0524,  ...,  0.0259, -0.0453,  0.0565],\n",
      "         [-0.0396, -0.0798, -0.0450,  ...,  0.0307, -0.0467,  0.0581]],\n",
      "\n",
      "        [[-0.0458, -0.0748, -0.0441,  ...,  0.0343, -0.0503,  0.0588],\n",
      "         [-0.0494, -0.0734, -0.0462,  ...,  0.0324, -0.0451,  0.0562],\n",
      "         [-0.0447, -0.0824, -0.0458,  ...,  0.0313, -0.0446,  0.0560],\n",
      "         [-0.0454, -0.0791, -0.0437,  ...,  0.0353, -0.0425,  0.0575]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0507, -0.0797, -0.0365,  ...,  0.0388, -0.0495,  0.0572],\n",
      "         [-0.0477, -0.0832, -0.0366,  ...,  0.0377, -0.0491,  0.0577],\n",
      "         [-0.0459, -0.0696, -0.0372,  ...,  0.0420, -0.0608,  0.0584],\n",
      "         [-0.0498, -0.0719, -0.0408,  ...,  0.0479, -0.0586,  0.0609]],\n",
      "\n",
      "        [[-0.0506, -0.0811, -0.0360,  ...,  0.0396, -0.0524,  0.0588],\n",
      "         [-0.0503, -0.0825, -0.0392,  ...,  0.0428, -0.0495,  0.0609],\n",
      "         [-0.0442, -0.0725, -0.0373,  ...,  0.0507, -0.0581,  0.0608],\n",
      "         [-0.0521, -0.0760, -0.0450,  ...,  0.0485, -0.0584,  0.0614]],\n",
      "\n",
      "        [[-0.0477, -0.0803, -0.0417,  ...,  0.0434, -0.0528,  0.0617],\n",
      "         [-0.0485, -0.0807, -0.0435,  ...,  0.0459, -0.0489,  0.0622],\n",
      "         [-0.0458, -0.0736, -0.0361,  ...,  0.0512, -0.0564,  0.0555],\n",
      "         [-0.0463, -0.0740, -0.0490,  ...,  0.0441, -0.0519,  0.0578]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "model state_h\n",
      "[3, 4, 128]\n",
      "tensor([[[ 0.0896, -0.0261,  0.0599,  ..., -0.1363,  0.3841,  0.0164],\n",
      "         [ 0.1657, -0.1122,  0.0972,  ...,  0.0735,  0.0454,  0.0112],\n",
      "         [ 0.1641, -0.0294,  0.1691,  ..., -0.1587,  0.0311, -0.1640],\n",
      "         [ 0.0763, -0.0298,  0.1172,  ..., -0.2956,  0.1862, -0.0689]],\n",
      "\n",
      "        [[ 0.0805,  0.0146,  0.0583,  ...,  0.0585, -0.0125,  0.0429],\n",
      "         [ 0.0648,  0.0095,  0.0372,  ...,  0.0621, -0.0077,  0.0078],\n",
      "         [ 0.0726, -0.0410,  0.0234,  ...,  0.0719, -0.0014, -0.0121],\n",
      "         [ 0.0669, -0.0242,  0.0345,  ...,  0.0983,  0.0069,  0.0097]],\n",
      "\n",
      "        [[-0.0025,  0.0107,  0.0083,  ..., -0.0345,  0.0055, -0.0223],\n",
      "         [-0.0178,  0.0065,  0.0159,  ..., -0.0346,  0.0046, -0.0082],\n",
      "         [-0.0117,  0.0136,  0.0143,  ..., -0.0329, -0.0005, -0.0131],\n",
      "         [-0.0322,  0.0133,  0.0077,  ..., -0.0328,  0.0094, -0.0110]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "model state_c\n",
      "[3, 4, 128]\n",
      "tensor([[[ 0.1175, -0.0663,  0.1796,  ..., -0.2752,  0.7639,  0.0707],\n",
      "         [ 0.3104, -0.2369,  0.2599,  ...,  0.2072,  0.0946,  0.0216],\n",
      "         [ 0.2448, -0.0496,  0.2743,  ..., -0.3259,  0.0527, -0.2679],\n",
      "         [ 0.1591, -0.0971,  0.3025,  ..., -0.6293,  0.5362, -0.1361]],\n",
      "\n",
      "        [[ 0.1550,  0.0280,  0.1294,  ...,  0.1248, -0.0240,  0.0859],\n",
      "         [ 0.1290,  0.0196,  0.0773,  ...,  0.1365, -0.0167,  0.0161],\n",
      "         [ 0.1531, -0.0773,  0.0500,  ...,  0.1540, -0.0030, -0.0226],\n",
      "         [ 0.1421, -0.0444,  0.0733,  ...,  0.1986,  0.0140,  0.0196]],\n",
      "\n",
      "        [[-0.0051,  0.0211,  0.0160,  ..., -0.0650,  0.0110, -0.0436],\n",
      "         [-0.0356,  0.0131,  0.0307,  ..., -0.0662,  0.0093, -0.0162],\n",
      "         [-0.0235,  0.0274,  0.0274,  ..., -0.0624, -0.0011, -0.0256],\n",
      "         [-0.0648,  0.0267,  0.0149,  ..., -0.0627,  0.0187, -0.0217]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "model loss\n",
      "[]\n",
      "tensor(8.8530, grad_fn=<NllLoss2DBackward>)\n",
      "predict x\n",
      "[1, 4]\n",
      "tensor([[176, 510, 993, 177]])\n",
      "predict y_pred\n",
      "[1, 4, 6925]\n",
      "tensor([[[ 0.3081,  0.2594,  0.2715,  ..., -0.2189, -0.3206, -0.3067],\n",
      "         [ 0.3638,  0.3117,  0.3112,  ..., -0.2598, -0.3551, -0.3591],\n",
      "         [ 0.2813,  0.2265,  0.2344,  ..., -0.1961, -0.2905, -0.2775],\n",
      "         [ 0.4539,  0.4036,  0.3979,  ..., -0.3195, -0.4357, -0.4521]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "predict state_h\n",
      "[3, 4, 128]\n",
      "tensor([[[-0.2998,  0.4996,  0.0518,  ..., -0.0597, -0.1409, -0.3807],\n",
      "         [ 0.0542,  0.0140,  0.2324,  ..., -0.0778,  0.0202,  0.0066],\n",
      "         [-0.1407,  0.0419,  0.2520,  ...,  0.0016,  0.1403, -0.0184],\n",
      "         [-0.2407,  0.3977,  0.0083,  ..., -0.0345,  0.0168,  0.1327]],\n",
      "\n",
      "        [[ 0.0305, -0.0017,  0.0177,  ...,  0.0008,  0.0109, -0.0063],\n",
      "         [ 0.1196,  0.0356,  0.0572,  ...,  0.0239,  0.0090,  0.0641],\n",
      "         [ 0.0601,  0.0365,  0.0365,  ...,  0.0164, -0.0235,  0.0564],\n",
      "         [ 0.0848,  0.0289,  0.0710,  ...,  0.0349, -0.0308,  0.0962]],\n",
      "\n",
      "        [[ 0.0291,  0.0331,  0.0523,  ...,  0.0214,  0.0405,  0.0245],\n",
      "         [ 0.0292,  0.0353,  0.0541,  ...,  0.0387,  0.0539,  0.0291],\n",
      "         [ 0.0252,  0.0271,  0.0356,  ...,  0.0260,  0.0353,  0.0237],\n",
      "         [ 0.0457,  0.0462,  0.0686,  ...,  0.0497,  0.0643,  0.0443]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "predict state_c\n",
      "[3, 4, 128]\n",
      "tensor([[[-0.4710,  0.6981,  0.2273,  ..., -0.1367, -0.2136, -0.5201],\n",
      "         [ 0.0969,  0.1010,  0.2994,  ..., -0.2725,  0.0744,  0.0143],\n",
      "         [-0.2674,  0.2079,  0.3607,  ...,  0.0035,  0.2740, -0.0677],\n",
      "         [-0.3203,  0.6340,  0.0649,  ..., -0.2620,  0.0442,  0.2077]],\n",
      "\n",
      "        [[ 0.0791, -0.0028,  0.0388,  ...,  0.0015,  0.0311, -0.0100],\n",
      "         [ 0.2106,  0.0798,  0.1041,  ...,  0.0446,  0.0220,  0.1147],\n",
      "         [ 0.1066,  0.0767,  0.0804,  ...,  0.0349, -0.0464,  0.1108],\n",
      "         [ 0.2348,  0.0424,  0.1555,  ...,  0.0546, -0.1139,  0.1281]],\n",
      "\n",
      "        [[ 0.0606,  0.0652,  0.0979,  ...,  0.0451,  0.0859,  0.0553],\n",
      "         [ 0.0614,  0.0685,  0.1011,  ...,  0.0825,  0.1167,  0.0652],\n",
      "         [ 0.0518,  0.0527,  0.0659,  ...,  0.0539,  0.0738,  0.0505],\n",
      "         [ 0.1028,  0.0965,  0.1340,  ...,  0.1126,  0.1572,  0.1159]]],\n",
      "       grad_fn=<StackBackward>)\n",
      "predict last_word_logits\n",
      "[6925]\n",
      "tensor([ 0.4539,  0.4036,  0.3979,  ..., -0.3195, -0.4357, -0.4521],\n",
      "       grad_fn=<SelectBackward>)\n",
      "predict p\n",
      "[0.00031003 0.00029482 0.00029316 ... 0.00014308 0.00012737 0.00012531]\n",
      "predict word_index\n",
      "3582\n",
      "predict dataset.index_to_word[word_index]\n",
      "strip?\n"
     ]
    }
   ],
   "source": [
    "step_logger.get_default_summary(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original : What did the bartender say to the jumper cables? You better not try to start anything.\n",
      "predicted : ['What', 'did', 'the', 'bartender', 'say', 'to', 'the', 'jumper', '', 'Houdini', 'Park', 'mama', 'knock', 'to', 'escaped', 'they', 'atoms']\n",
      "predicted with max : ['What', 'did', 'the', 'bartender', 'say', 'to', 'the', 'jumper', '', 'and', 'the', 'the', 'What', 'do', 'the', 'the', 'What']\n",
      " - \n",
      "original : Don't you hate jokes about German sausage? They're the wurst!\n",
      "predicted : [\"Don't\", 'you', 'hate', 'jokes', 'about', '', 'pretend', 'local', 'out;', 'electrical', 'as']\n",
      "predicted with max : [\"Don't\", 'you', 'hate', 'jokes', 'about', '', 'and', 'the', 'the', 'What', 'do']\n",
      " - \n",
      "original : Two artists had an art contest... It ended in a draw\n",
      "predicted : ['Two', 'artists', 'had', 'an', 'art', '', 'why', 'hose', 'aircraft.', 'mummies?', 'hotel...']\n",
      "predicted with max : ['Two', 'artists', 'had', 'an', 'art', '', 'and', 'the', 'the', 'What', 'do']\n",
      " - \n",
      "original : Why did the chicken cross the playground? To get to the other slide.\n",
      "predicted : ['Why', 'did', 'the', 'chicken', 'cross', 'the', '', 'Aonther', 'egg?', 'elephants', 'All', 'he', 'collection?']\n",
      "predicted with max : ['Why', 'did', 'the', 'chicken', 'cross', 'the', '', 'and', 'the', 'the', 'What', 'do', 'the']\n",
      " - \n",
      "original : What gun do you use to hunt a moose? A moosecut!\n",
      "predicted : ['What', 'gun', 'do', 'you', 'use', '', 'Just', 'old.', 'naybor', 'about', 'for']\n",
      "predicted with max : ['What', 'gun', 'do', 'you', 'use', '', 'and', 'the', 'the', 'What', 'do']\n",
      " - \n"
     ]
    }
   ],
   "source": [
    "#'Knock knock. Whos there?'\n",
    "def predict_for_word(index):\n",
    "    sentence = train_df.iloc[index][1]\n",
    "    half_len= len(sentence.split( \" \")) //2\n",
    "    half_sentence = \" \".join(sentence.split( \" \")[0:half_len]) + \" \"\n",
    "\n",
    "    print(\"original :\",sentence)\n",
    "    print(\"predicted :\",predict(dataset, model, text=half_sentence,next_words=half_len))\n",
    "    print(\"predicted with max :\",predict(dataset, model, text=half_sentence,next_words=half_len,use_max_possible=True))\n",
    "    \n",
    "    print(\" - \")\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    predict_for_word(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
